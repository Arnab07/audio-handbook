{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c11441",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYWORLD-BASED SPEECH ANALYSIS PIPELINE\n",
    "# ============================================================\n",
    "# This version replaces custom cepstrum + harmonic heuristics (Cepstrum ≠ pitch tracker)\n",
    "# with a robust, production-grade approach using pyworld.\n",
    "# | System              | What “pitch” means                            |\n",
    "# | ------------------- | --------------------------------------------- |\n",
    "# | **Cepstrum-based**  | Dominant periodic spacing in the spectrum     |\n",
    "# | **WORLD (pyworld)** | Estimated vocal fold vibration (F₀) over time |\n",
    "# Cepstrum often locks onto:2nd harmonic (2×F₀)or even formant spacing artifacts\n",
    "# WORLD estimates glottal excitation, not spectral repetition.Models vocal fold vibration, Suppresses harmonics\n",
    "# So WORLD’s pitch is the physical F₀.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pyworld as pw\n",
    "from scipy.signal import welch\n",
    "\n",
    "# ============================================================\n",
    "# Core Feature Extraction\n",
    "# ============================================================\n",
    "\n",
    "def extract_f0(audio, sr):\n",
    "    \"\"\"\n",
    "    Extract fundamental frequency (F0) using WORLD.\n",
    "    Returns f0 (Hz) and time axis.\n",
    "    \"\"\"\n",
    "    f0, t = pw.dio(audio, sr)\n",
    "    f0 = pw.stonemask(audio, f0, t, sr)\n",
    "    return f0, t\n",
    "\n",
    "\n",
    "def spectral_entropy(signal, sr, n_fft=2048):\n",
    "    spec = np.abs(np.fft.rfft(signal, n=n_fft))\n",
    "    spec = spec / (np.sum(spec) + 1e-12)\n",
    "    return -np.sum(spec * np.log2(spec + 1e-12))\n",
    "\n",
    "\n",
    "def spectral_flatness(signal, n_fft=2048):\n",
    "    spectrum = np.abs(np.fft.rfft(signal, n=n_fft)) + 1e-12\n",
    "    return np.exp(np.mean(np.log(spectrum))) / np.mean(spectrum)\n",
    "\n",
    "# ============================================================\n",
    "# Pitch statistics and voicing\n",
    "# ============================================================\n",
    "\n",
    "def analyze_pitch(audio, sr):\n",
    "    f0, t = extract_f0(audio, sr)\n",
    "\n",
    "    voiced = f0 > 0\n",
    "    voiced_ratio = np.mean(voiced)\n",
    "\n",
    "    if np.any(voiced):\n",
    "        pitch_std = np.std(f0[voiced])\n",
    "        pitch_median = np.median(f0[voiced])\n",
    "    else:\n",
    "        pitch_std = 0.0\n",
    "        pitch_median = np.nan\n",
    "\n",
    "    return {\n",
    "        \"f0\": f0,\n",
    "        \"voiced_ratio\": voiced_ratio,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"pitch_median\": pitch_median,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# High-level classification logic\n",
    "# ============================================================\n",
    "\n",
    "def classify_signal(audio, sr):\n",
    "    # Pitch-based features\n",
    "    pitch_info = analyze_pitch(audio, sr)\n",
    "\n",
    "    # Spectral features\n",
    "    entropy = spectral_entropy(audio, sr)\n",
    "    flatness = spectral_flatness(audio)\n",
    "\n",
    "    voiced_ratio = pitch_info[\"voiced_ratio\"]\n",
    "    pitch_std = pitch_info[\"pitch_std\"]\n",
    "\n",
    "    # Decision logic (robust + interpretable)\n",
    "    if voiced_ratio < 0.2:\n",
    "        decision = \"unvoiced_or_noise\"\n",
    "    elif pitch_std < 5:\n",
    "        decision = \"periodic_non_speech\"\n",
    "    elif 0.25 < flatness < 0.8 and 4 < entropy < 9:\n",
    "        decision = \"voiced_speech\"\n",
    "    else:\n",
    "        decision = \"ambiguous\"\n",
    "\n",
    "    return {\n",
    "        \"decision\": decision,\n",
    "        \"pitch_median\": pitch_info[\"pitch_median\"],\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"voiced_ratio\": voiced_ratio,\n",
    "        \"entropy\": entropy,\n",
    "        \"flatness\": flatness,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Example usage\n",
    "# ============================================================\n",
    "result = classify_signal(noisy_audio, sample_rate)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
